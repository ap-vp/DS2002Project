{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f56c31ce-a874-4063-8bab-6c2835327d45",
   "metadata": {},
   "source": [
    " ETL pipeline for DS-2002 Insurance Data Mart\n",
    "\n",
    "- Extracts:\n",
    "    1) File system: data/insurance.csv\n",
    "    2) Semi-structured \"NoSQL-like\": data/providers.json\n",
    "    3) Relational SQL: executes classicmodels SQL to a local SQLite DB (sources/classicmodels.db) and\n",
    "       extracts a tiny reference table (offices) just to prove SQL extraction. \n",
    "\n",
    "- Transforms:\n",
    "    * Derives age bands and BMI categories\n",
    "    * Normalizes region names\n",
    "    * Generates a reproducible synthetic claim_date to satisfy the Date dimension requirement\n",
    "    * Reduces/reshapes columns (modifies number of columns from source to destination)\n",
    "\n",
    "- Loads:\n",
    "    * Populates star schema tables in a local SQLite database: data_mart.db\n",
    "    * Runs sample analytic queries from sql/sample_queries.sql\n",
    "\n",
    "Run:\n",
    "    python etl/etl_pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c911a-75e3-42dc-9268-7f7907db8b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, json, sqlite3, random, hashlib\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(__file__))\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "SQL_DIR = os.path.join(PROJECT_ROOT, \"sql\")\n",
    "SOURCES_DIR = os.path.join(PROJECT_ROOT, \"sources\")\n",
    "MART_DB = os.path.join(PROJECT_ROOT, \"data_mart.db\")\n",
    "CLASSICMODELS_DB = os.path.join(SOURCES_DIR, \"classicmodels.db\")\n",
    "\n",
    "def exec_sql_script(conn: sqlite3.Connection, path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sql = f.read()\n",
    "    conn.executescript(sql)\n",
    "\n",
    "def build_classicmodels_sqlite():\n",
    "    \"\"\"Load provided MySQL sample database into a local SQLite DB (best effort).\"\"\"\n",
    "    sql_path = os.path.join(SOURCES_DIR, \"mysqlsampledatabase.sql\")\n",
    "    if not os.path.exists(sql_path):\n",
    "        print(\"classicmodels SQL not found; skipping SQL extraction step.\")\n",
    "        return\n",
    "    conn = sqlite3.connect(CLASSICMODELS_DB)\n",
    "    conn.execute(\"PRAGMA foreign_keys = ON;\")\n",
    "    try:\n",
    "        exec_sql_script(conn, sql_path)\n",
    "        conn.commit()\n",
    "        try:\n",
    "            df = pd.read_sql_query(\"SELECT officeCode, city, country, territory FROM offices;\", conn)\n",
    "            df.to_csv(os.path.join(DATA_DIR, \"classicmodels_offices_extract.csv\"), index=False)\n",
    "            print(f\"Extracted {len(df)} office rows from SQL source.\")\n",
    "        except Exception as e:\n",
    "            print(\"Extraction query failed (likely due to SQLite type quirks), continuing. Error:\", e)\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def make_date_dim(start=\"2010-01-01\", end=\"2010-12-31\"):\n",
    "    start_dt = datetime.fromisoformat(start)\n",
    "    end_dt = datetime.fromisoformat(end)\n",
    "    rows = []\n",
    "    cur = start_dt\n",
    "    while cur <= end_dt:\n",
    "        date_key = int(cur.strftime(\"%Y%m%d\"))\n",
    "        rows.append({\n",
    "            \"date_key\": date_key,\n",
    "            \"full_date\": cur.date().isoformat(),\n",
    "            \"day_of_week\": cur.isoweekday(),\n",
    "            \"day_name\": cur.strftime(\"%A\"),\n",
    "            \"month_of_year\": cur.month,\n",
    "            \"month_name\": cur.strftime(\"%B\"),\n",
    "            \"calendar_quarter\": (cur.month - 1)//3 + 1,\n",
    "            \"calendar_year\": cur.year,\n",
    "            \"is_weekend\": 1 if cur.weekday() >= 5 else 0,\n",
    "        })\n",
    "        cur += timedelta(days=1)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def age_band(age:int)->str:\n",
    "    bins = [(0,17,\"0-17\"),(18,24,\"18-24\"),(25,34,\"25-34\"),(35,44,\"35-44\"),\n",
    "            (45,54,\"45-54\"),(55,64,\"55-64\"),(65,200,\"65+\")]\n",
    "    for lo,hi,label in bins:\n",
    "        if lo <= age <= hi:\n",
    "            return label\n",
    "    return \"unknown\"\n",
    "\n",
    "def bmi_category(bmi:float)->str:\n",
    "    if bmi < 18.5: return \"Underweight\"\n",
    "    if bmi < 25: return \"Normal\"\n",
    "    if bmi < 30: return \"Overweight\"\n",
    "    return \"Obese\"\n",
    "\n",
    "def deterministc_date_for_row(row_id:int, year:int=2010)->int:\n",
    "    # Stable date per row using a hash; spreads across the year\n",
    "    rnd = int(hashlib.sha256(str(row_id).encode()).hexdigest(), 16)\n",
    "    day_of_year = (rnd % 365) + 1\n",
    "    dt = datetime(year, 1, 1) + timedelta(days=day_of_year - 1)\n",
    "    return int(dt.strftime(\"%Y%m%d\"))\n",
    "\n",
    "def main():\n",
    "    # 1) Build a local classicmodels.db from the provided SQL (relational source)\n",
    "    build_classicmodels_sqlite()\n",
    "\n",
    "    # 2) Read file sources\n",
    "    ins_path = os.path.join(DATA_DIR, \"insurance.csv\")\n",
    "    providers_path = os.path.join(DATA_DIR, \"providers.json\")\n",
    "    insurance = pd.read_csv(ins_path)\n",
    "    with open(providers_path) as f:\n",
    "        providers_doc = json.load(f)\n",
    "    providers = pd.DataFrame(providers_doc[\"regions\"])\n",
    "\n",
    "    # 3) Transform insurance to dimensions\n",
    "    insured = insurance.copy()\n",
    "    insured[\"age_band\"] = insured[\"age\"].apply(age_band)\n",
    "    insured[\"bmi_category\"] = insured[\"bmi\"].apply(bmi_category)\n",
    "\n",
    "    # Normalize region strings\n",
    "    insured[\"region\"] = insured[\"region\"].str.strip().str.lower()\n",
    "\n",
    "    # 4) Build region dim (left-join to providers.json to demonstrate semi-structured enrichment)\n",
    "    regions = insured[[\"region\"]].drop_duplicates().merge(providers, on=\"region\", how=\"left\")\n",
    "    regions = regions.sort_values(\"region\").reset_index(drop=True)\n",
    "    regions[\"region_key\"] = regions.index + 1\n",
    "\n",
    "    # 5) Build insured dim with surrogate keys\n",
    "    insured_dim = insured[[\"age\",\"age_band\",\"sex\",\"smoker\",\"bmi\",\"bmi_category\",\"children\"]].drop_duplicates().reset_index(drop=True)\n",
    "    insured_dim[\"insured_key\"] = insured_dim.index + 1\n",
    "\n",
    "    # 6) Build date dimension\n",
    "    dim_date = make_date_dim(\"2010-01-01\",\"2010-12-31\")\n",
    "\n",
    "    # 7) Build fact table: assign insured_key & region_key; synthetic date for requirement\n",
    "    ins_enriched = insured.merge(insured_dim, on=[\"age\",\"age_band\",\"sex\",\"smoker\",\"bmi\",\"bmi_category\",\"children\"], how=\"left\")\n",
    "    ins_enriched = ins_enriched.merge(regions[[\"region\",\"region_key\"]], on=\"region\", how=\"left\")\n",
    "\n",
    "    ins_enriched[\"row_id\"] = np.arange(1, len(ins_enriched)+1)\n",
    "    ins_enriched[\"date_key\"] = ins_enriched[\"row_id\"].apply(lambda i: deterministc_date_for_row(i, 2010))\n",
    "\n",
    "    fact = ins_enriched[[\"insured_key\",\"region_key\",\"date_key\",\"charges\"]].copy()\n",
    "    fact[\"claim_key\"] = np.arange(1, len(fact)+1)\n",
    "\n",
    "    # 8) Load into SQLite mart\n",
    "    if os.path.exists(MART_DB): os.remove(MART_DB)\n",
    "    conn = sqlite3.connect(MART_DB)\n",
    "    conn.execute(\"PRAGMA foreign_keys = ON;\")\n",
    "    conn.executescript(open(os.path.join(SQL_DIR, \"create_data_mart.sql\")).read())\n",
    "    dim_date.to_sql(\"dim_date\", conn, if_exists=\"append\", index=False)\n",
    "    insured_dim[[\"insured_key\",\"age\",\"age_band\",\"sex\",\"smoker\",\"bmi\",\"bmi_category\",\"children\"]].to_sql(\"dim_insured\", conn, if_exists=\"append\", index=False)\n",
    "    regions[[\"region_key\",\"region\",\"preferred_provider\",\"network_tier\"]].to_sql(\"dim_region\", conn, if_exists=\"append\", index=False)\n",
    "    fact[[\"claim_key\",\"insured_key\",\"region_key\",\"date_key\",\"charges\"]].to_sql(\"fact_claims\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "    # 9) Run sample queries to verify\n",
    "    print(\"Row counts:\")\n",
    "    for tbl in [\"dim_date\",\"dim_insured\",\"dim_region\",\"fact_claims\"]:\n",
    "        cur = conn.execute(f\"SELECT COUNT(*) FROM {tbl}\")\n",
    "        print(f\"  {tbl}: {cur.fetchone()[0]}\")\n",
    "\n",
    "    print(\"\\nSample query: total charges by smoker & region in 2010 (top 5 rows):\")\n",
    "    q1 = \"\"\"\n",
    "    SELECT d.calendar_year, i.smoker, r.region, ROUND(SUM(f.charges),2) AS total_charges\n",
    "    FROM fact_claims f\n",
    "    JOIN dim_insured i ON f.insured_key = i.insured_key\n",
    "    JOIN dim_region r  ON f.region_key = r.region_key\n",
    "    JOIN dim_date d    ON f.date_key   = d.date_key\n",
    "    GROUP BY d.calendar_year, i.smoker, r.region\n",
    "    ORDER BY total_charges DESC\n",
    "    LIMIT 5;\n",
    "    \"\"\"\n",
    "    print(pd.read_sql_query(q1, conn))\n",
    "\n",
    "    conn.close()\n",
    "    print(f\"\\nAll done. SQLite mart at: {MART_DB}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ec4cb9-541c-49b4-8792-33fe22627ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pysparkenv]",
   "language": "python",
   "name": "conda-env-pysparkenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
